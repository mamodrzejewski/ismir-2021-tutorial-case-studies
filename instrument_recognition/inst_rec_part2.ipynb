{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inst_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure model can overfit on limited dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERFIT_CONFIG = dict(training_set_limit=2,\n",
    "                      validation_set_limit=100,\n",
    "                      learning_rate=5e-3,\n",
    "                      steps_per_epoch=1000,\n",
    "                      buffer_size=8,\n",
    "                      batch_size=8,\n",
    "                      epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_rec.train_model(OVERFIT_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = dict(training_set_limit=-1,\n",
    "                      validation_set_limit=1000,\n",
    "                      learning_rate=1e-3,\n",
    "                      steps_per_epoch=100,\n",
    "                      buffer_size=1000,\n",
    "                      batch_size=32,\n",
    "                      epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_rec.train_model(DEFAULT_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = inst_rec.build_model()\n",
    "best_model.load_weights(inst_rec.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a random track from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mirdata, librosa\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msdb = mirdata.initialize('medley_solos_db', data_home=inst_rec.MIRDATA_MDSB_PATH)\n",
    "track_ids = [t_id for t_id in msdb.track_ids if msdb.track(t_id).subset=='test']\n",
    "random.shuffle(track_ids)\n",
    "track_id = track_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = msdb.track(track_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(track.audio_path, sr=inst_rec.SR)\n",
    "x = librosa.util.frame(audio, frame_length=inst_rec.SR, hop_length=inst_rec.SR // 2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = best_model.predict(x[:,np.newaxis,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(np.mean(l, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUMENTS = ['clarinet', \n",
    "               'distorted electric guitar', \n",
    "               'female singer',\n",
    "               'flute',\n",
    "               'piano', \n",
    "               'tenor saxophone', \n",
    "               'trumpet', \n",
    "               'violin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(audio, rate=sr))\n",
    "INSTRUMENTS[y_pred], INSTRUMENTS[track.instrument_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval on test\n",
    "(Note this function is pretty inefficient since it only predicts one track at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test(model):\n",
    "    msdb = mirdata.initialize('medley_solos_db', data_home=MIRDATA_MDSB_PATH)\n",
    "    track_ids = [t_id for t_id in msdb.track_ids if msdb.track(t_id).subset=='test']\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for track_id in tqdm(track_ids):\n",
    "        track = msdb.track(track_id)\n",
    "        audio, sr = librosa.load(track.audio_path, sr=SR)\n",
    "        x = librosa.util.frame(audio, frame_length=SR, hop_length=SR // 2).T\n",
    "        l = model.predict(x[:,np.newaxis,:])\n",
    "        _y_pred = np.argmax(np.mean(l, axis=0))\n",
    "        y_pred.append(_y_pred)\n",
    "        y_true.append(track.instrument_id)\n",
    "\n",
    "    print(sklearn.metrics.classification_report(y_true, y_pred))\n",
    "    \n",
    "    matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "    print(np.mean(matrix.diagonal()/matrix.sum(axis=1)))\n",
    "        \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = eval_on_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune model\n",
    "Note this hasn not been optimized and will likely overfit. To help reduce that try:\n",
    "1. Augmenting the dataset (e.g., small pitch shifts, time stretch/compression, added noise, compression, etc.)\n",
    "1. Limiting fine-tuning to last layer (...though the last layer does have most of the weights)\n",
    "1. Reducing learning rate (or try using lower learning rates for just earlier layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = inst_rec.build_model(ol3_trainable=True)\n",
    "best_model.load_weights(inst_rec.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_CONFIG = dict(training_set_limit=-1,\n",
    "                      validation_set_limit=1000,\n",
    "                      learning_rate=1e-5,\n",
    "                      steps_per_epoch=100,\n",
    "                      buffer_size=1000,\n",
    "                      batch_size=32,\n",
    "                      epochs=10)\n",
    "\n",
    "inst_rec.train_model(model=best_model, config=FINETUNE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
